from tensor import Tensor, TensorSpec, TensorShape
from algorithm import parallelize, vectorize
from utils.index import Index
from random import randn, random_si64, seed
from pathlib import path
from math import exp, pow, rsqrt
from python import Python
from linalg import transpose, matmul

alias type = DType.float64
alias simdwidth = simdwidthof[type]()
alias mu: Float64 = .1
alias beta1: Float64 = 0.9
alias beta2: Float64 = 0.99
alias epsilon: Float64 = 0.00000001
alias mini_batch_size: Int = 25
alias test_batch_size: Int = 100
alias epochs: Int = 75000
alias error_target: Float64 = .075
alias input_layer_size: Int = 16
alias hidden_layer_size: Int = 32
alias output_layer_size: Int = 26
alias data_width: Int = 17
alias data_size: Int = 20000
alias training_size: Int = 16000
alias validation_size: Int = 4000

# 1.0/(1.0 + exp(-x))
fn sigmoid(z: Tensor[type]) -> Tensor[type]:
    var activations: Tensor[type] = Tensor[type](z.shape())

    @parameter
    fn compute_exp[simd_width: Int](idx: Int):
        activations.simd_store[simd_width](idx, (1 / (1 + exp[type, simd_width](-1 * z.simd_load[simd_width](idx)))))
    
    vectorize[simdwidth, compute_exp](activations.num_elements())
    
    return activations

# 1/sqrt(x + epsilon)
fn inv_sqrt(z: Tensor[type]) -> Tensor[type]:
    var second_moments: Tensor[type] = Tensor[type](z.shape())

    @parameter
    fn compute_rsqrt[simd_width: Int](idx: Int):
        second_moments.simd_store[simd_width](idx, rsqrt[type, simd_width](epsilon + z.simd_load[simd_width](idx)))
    
    vectorize[simdwidth, compute_rsqrt](second_moments.num_elements())
    
    return second_moments


# sigmoid(z) * (1 - sigmoid(z))
fn sigmoid_prime(a: Tensor[type]) raises -> Tensor[type]:
    var sigma_prime: Tensor[type] = Tensor[type](a.shape()) 

    sigma_prime = a * (1 - a)

    return sigma_prime

fn sigmoid_prime_full(z: Tensor[type]) raises -> Tensor[type]:
    var sigma_prime: Tensor[type] = Tensor[type](z.shape()) 
    let sigmoid_z = sigmoid(z)

    sigma_prime = sigmoid_z * (1 - sigmoid_z)

    return sigma_prime

fn feed_forward():
    return

fn output_error(a_L: Tensor[type], expected: Tensor[type], a_L_prime: Tensor[type]) raises -> Tensor[type]:
    var error_L: Tensor[type] = Tensor[type](a_L.shape())

    error_L = (a_L - expected) * a_L_prime

    return error_L

fn backpropagation(w: Tensor[type], error: Tensor[type], a_prime: Tensor[type]) raises -> Tensor[type]:
    var error_l: Tensor[type] = Tensor[type](TensorShape(mini_batch_size, hidden_layer_size))
    var w_transpose: Tensor[type] = transpose(w) 

    error_l = a_prime * matmul(error, w_transpose)

    return error_l

fn adamopt(inout d_L: Tensor[type], inout d_l: Tensor[type], inout d_L_m: Tensor[type], inout d_l_m: Tensor[type], inout d_L_v: Tensor[type], inout d_l_v: Tensor[type], epoch: Int) raises:
    d_L_m = beta1 * d_L_m + (1-beta1) * d_L
    d_l_m = beta1 * d_l_m + (1-beta1) * d_l

    d_L_v = beta2 * d_L_v + (1-beta2) * d_L * d_L
    d_l_v = beta2 * d_l_v + (1-beta2) * d_l * d_l

    var d_L_mhat = d_L_m / (1 - pow(beta1, epoch+1))
    var d_l_mhat = d_l_m / (1 - pow(beta1, epoch+1))

    var d_L_vhat = d_L_v / (1 - pow(beta2, epoch+1))
    var d_l_vhat = d_l_v / (1 - pow(beta2, epoch+1))

    var L_update = d_L_mhat * inv_sqrt(d_L_vhat)
    var l_update = d_l_mhat * inv_sqrt(d_l_vhat)

    d_L = L_update
    d_l = l_update

fn update_weights(inout w: Tensor[type], error: Tensor[type], a_prev: Tensor[type]) raises:
    let a_transpose: Tensor[type] = transpose(a_prev) 
    
    w = w - (mu / mini_batch_size) * matmul(a_transpose, error)

fn update_biases(inout b: Tensor[type], error: Tensor[type]) raises:
    b = b - (mu / mini_batch_size) * error

fn softmax(inout a: Tensor[type]):
    var soft: Tensor[type] = Tensor[type](a.shape())

    @parameter
    fn soft_exp[simd_width: Int](idx: Int):
        a.simd_store[simd_width](idx, exp[type, simd_width](a.simd_load[simd_width](idx)))

    vectorize[simdwidth, soft_exp](a.num_elements())

    # @parameter
    # fn softmax_tasks(row: Int):
    #     var row_sum: Float64 = 0.0

    #     for j in range(a.shape()[1]):
    #         row_sum += a[Index(row,j)]    
    #     for k in range(a.shape()[1]):
    #         a[Index(row,k)] /= row_sum

    # @parameter
    # fn softmax_tasks_parallel():
    #     parallelize[softmax_tasks](a.shape()[0], a.shape()[0])

    for i in range(a.shape()[0]):
        var row_sum: Float64 = 0.0

        for j in range(a.shape()[1]):
            row_sum += a[Index(i,j)]    
        for k in range(a.shape()[1]):
            a[Index(i,k)] /= row_sum 
    
fn set_training_indices(inout indices: Tensor[DType.int64]):
    for i in range(mini_batch_size):
        indices[Index(0,i)] = random_si64(0,training_size)

fn set_test_indices(inout indices: Tensor[DType.int64]):
    for i in range(test_batch_size):
        indices[Index(0,1)] = random_si64(training_size + 1, data_size)

def get_data() -> Tensor[type]:
    var data_input = Tensor[type](TensorSpec(type, data_size, data_width))
    let np = Python.import_module("numpy")
    
    test = np.genfromtxt("letters-data-normalized.txt", np.float64)

    for i in range(data_size):
        for j in range(data_width):
            data_input[Index(i,j)] = test[i][j].to_float64()
    return data_input 

fn get_validation() -> Tensor[type]:
    var check = Tensor[type](TensorSpec(type, output_layer_size, output_layer_size))

    for i in range(check.shape()[0]):
        for j in range(check.shape()[1]):
            if(i == j):
                check[Index(i,j)] = 0.999
            else:
                check[Index(i,j)] = 0.001
    
    return check

fn set_batch_and_validation(indices: Tensor[DType.int64], data: Tensor[type], checker: Tensor[type], inout x: Tensor[type], inout v: Tensor[type]):
    # indices num elements = mini_batch_size
    var v_letter: Int 
    for i in range(indices.num_elements()):
        for j in range(input_layer_size):
            x[Index(i,j)] = data[Index(indices[i], j + 1)]

        for k in range(output_layer_size):
            v_letter = data[Index(indices[i],0)].to_int()
            v[Index(i,k)] = checker[Index(v_letter,k)]


fn get_batch_error(activations: Tensor[type], checker: Tensor[type]) -> Float64:
    var error: Float64 = 0.0
    var num_incorrect: Int = 0
    var a_idx: Int = 0
    var c_idx: Int = 0
    var a_max: Float64 = 0.0
    var c_max: Float64 = 0.0

    for i in range(activations.shape()[0]):
        for j in range(activations.shape()[1]):
            if activations[Index(i,j)] > a_max:
                a_max = activations[Index(i,j)]
                a_idx = j
        for k in range(checker.shape()[1]):
            if checker[Index(i,k)] > c_max:
                c_max = checker[Index(i,k)]
                c_idx = k

        if a_idx != c_idx:
            num_incorrect += 1

        a_idx = 0
        c_idx = 0
        a_max = 0.0
        c_max = 0.0
     
    error = num_incorrect / mini_batch_size
    return error

# input activations
# feed forward
# output error
# backpropagation of error
# gradient descent to update weights
fn main() raises:
    seed()
    print("learning rate: ", mu, " error target: ", error_target)
    print("mini-batch size: ", mini_batch_size, " number of epochs: ", epochs)
    print("input size: ", input_layer_size," hidden layer size: ", hidden_layer_size, " output size: ", output_layer_size)
    print("\n\nloading data...")

    let adam_optimize: Bool = True 
    var batch_error: Float64 = 1.0
    let full_data: Tensor[type] = get_data()
    let output_check: Tensor[type] = get_validation() 

    print("\n\nconfiguring network...")
    let X_specs = TensorSpec(type, mini_batch_size, input_layer_size)

    let W_l_specs = TensorSpec(type, input_layer_size, hidden_layer_size)
    let W_L_specs = TensorSpec(type, hidden_layer_size, output_layer_size)

    let a_l_specs = TensorSpec(type, mini_batch_size, hidden_layer_size)
    let a_L_specs = TensorSpec(type, mini_batch_size, output_layer_size)

    let B_l_specs = TensorSpec(type, mini_batch_size, hidden_layer_size)
    let B_L_specs = TensorSpec(type, mini_batch_size, output_layer_size)

    var batch_indices: Tensor[DType.int64] = Tensor[DType.int64](TensorShape(1, mini_batch_size)) 
    var X: Tensor[type] = Tensor[type](X_specs)
    var validation: Tensor[type] = Tensor[type](a_L_specs)

    var W_l: Tensor[type] = randn[type](W_l_specs, 0, 1)
    var B_L: Tensor[type] = randn[type](B_L_specs, 0, 1)
    var W_L: Tensor[type] = randn[type](W_L_specs, 0, 1)
    var B_l: Tensor[type] = randn[type](B_l_specs, 0, 1)

    var z_l = Tensor[type](a_l_specs)
    var a_l = Tensor[type](a_l_specs)
    var a_l_prime = Tensor[type](a_l_specs)

    var z_L = Tensor[type](a_L_specs)
    var a_L = Tensor[type](a_L_specs)
    var a_L_prime = Tensor[type](a_L_specs)

    var d_l = Tensor[type](a_l_specs)
    var d_L = Tensor[type](a_L_specs)

    # EMA means
    var d_L_m = Tensor[type](a_L_specs)
    var d_l_m = Tensor[type](a_l_specs)

    # EMA variances
    var d_L_v = Tensor[type](a_L_specs)
    var d_l_v = Tensor[type](a_l_specs)

    print("\n\ntraining...")
    if adam_optimize: print("using adam optimization")

    # @unroll(mini_batch_size)
    # for i in range(epochs):
    var i = 0
    while batch_error > error_target:
        set_training_indices(batch_indices)
        set_batch_and_validation(batch_indices, full_data, output_check, X, validation)

        z_l = matmul(X, W_l) + B_l
        a_l = sigmoid(z_l)
        a_l_prime = sigmoid_prime(a_l)

        z_L = matmul(a_l, W_L) + B_L
        a_L = sigmoid(z_L)
        a_L_prime = sigmoid_prime(a_L)
        d_L = output_error(a_L, validation, a_L_prime)
        d_l = backpropagation(W_L, d_L, a_l_prime)

        if adam_optimize:
            adamopt(d_L, d_l, d_L_m, d_l_m, d_L_v, d_l_v, i)
        
        update_weights(W_L, d_L, a_l)
        update_weights(W_l, d_l, X)
        update_biases(B_L, d_L)
        update_biases(B_l, d_l)

        # calculate batch error
        softmax(a_L)
        batch_error = get_batch_error(a_L, validation)
        if i % 5000 == 0:
            print("epoch ", i)
            print("batch error: ", (batch_error * 100),"%")

        seed()
        if batch_error <= error_target:
            print("epoch ", i)
            print("batch error: ", (batch_error * 100),"%")
        i += 1

    print("training done")
    print("\n\ntesting...")
    # test here
    for i in range(10):
        set_test_indices(batch_indices)
        set_batch_and_validation(batch_indices, full_data, output_check, X, validation)
        z_l = matmul(X, W_l) + B_l
        a_l = sigmoid(z_l)
        a_l_prime = sigmoid_prime(a_l)

        z_L = matmul(a_l, W_L) + B_L
        a_L = sigmoid(z_L)
        softmax(a_L)
        batch_error = get_batch_error(a_L, validation)

        print("test error: ", (batch_error * 100),"%")